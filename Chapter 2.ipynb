{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: A/B Testing: Evaluating a Change to the System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi']= 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that Python converts between booleans and floats like this:\n",
    "print (float(True), float(False))\n",
    "print (bool(0), bool(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr1 = \"#333333\"\n",
    "clr2 = \"#777777\"\n",
    "clr3 = \"#AAAAAA\"\n",
    "\n",
    "def save_fig(fig_num):\n",
    "    pass\n",
    "    # fig_dir = \"/Users/dsweet2/Desktop/Tuning Up/Chapter 2/\"\n",
    "    #for ext in [\"eps\", \"png\"]:\n",
    "    #    plt.savefig(f\"{fig_dir}/CH02_FIG_{fig_num}_sweet.{ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizonal_line(y0):\n",
    "    c = plt.axis()\n",
    "    plt.plot([c[0], c[1]], [y0, y0], '--', linewidth=1, color=clr3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\tRandomize to Isolate the change being tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(strategy_A, server_1):\n",
    "    return 10 + float(strategy_A) - 2*float(server_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost(strategy_A=True, server_1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biased_experiment():\n",
    "    cost_A = cost(strategy_A=True, server_1=True)\n",
    "    cost_B = cost(strategy_A=False, server_1=False)\n",
    "\n",
    "    return cost_B - cost_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbiased_experiment():\n",
    "    cost_A_1 = cost(strategy_A=True, server_1=True)\n",
    "    cost_A_2 = cost(strategy_A=True, server_1=False)\n",
    "    cost_B_1 = cost(strategy_A=False, server_1=False)\n",
    "    cost_B_2 = cost(strategy_A=False, server_1=True)    \n",
    "    cost_A = (cost_A_1 + cost_A_2)/2\n",
    "    cost_B = (cost_B_1 + cost_B_2)/2\n",
    "    \n",
    "    return cost_B - cost_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_experiment():\n",
    "    cost_A = cost(strategy_A=True, server_1=bool(np.random.randint(2)))\n",
    "    cost_B = cost(strategy_A=False, server_1=bool(np.random.randint(2)))\n",
    "            \n",
    "    return cost_B - cost_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "print (randomized_experiment())\n",
    "print (randomized_experiment())\n",
    "print (randomized_experiment())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expectation():\n",
    "    data = []\n",
    "    for _ in range(1000):\n",
    "        data.append(randomized_experiment())\n",
    "    data = np.array(data) \n",
    "    return data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17); calculate_expectation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice([-1,1], size=(10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_complex(strategy_A, nuisance_factors):\n",
    "    return float(strategy_A) + nuisance_factors.sum()/20\n",
    "\n",
    "def randomized_experiment_complex(num_nuisance_factors):\n",
    "    nuisance_factors_A = np.random.choice([-1,1],\n",
    "                                          size=(num_nuisance_factors,))\n",
    "    cost_A = cost_complex(True, nuisance_factors_A)\n",
    "    nuisance_factors_B = np.random.choice([-1,1],\n",
    "                                          size=(num_nuisance_factors,))\n",
    "    cost_B = cost_complex(False, nuisance_factors_B)\n",
    "            \n",
    "    return cost_B - cost_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "print (randomized_experiment_complex(num_nuisance_factors=100))\n",
    "print (randomized_experiment_complex(num_nuisance_factors=100))\n",
    "print (randomized_experiment_complex(num_nuisance_factors=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expectation_complex(num_nuisance_factors):\n",
    "    data = [randomized_experiment_complex(num_nuisance_factors)\n",
    "            for _ in range(1000)]\n",
    "    return np.array(data).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17);\n",
    "print (calculate_expectation_complex(num_nuisance_factors=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\tReplicate and determine the number of measurements to take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1\tRandomization induces variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17);\n",
    "data_rec_10000 = np.array([\n",
    "    randomized_experiment_complex(num_nuisance_factors=100)\n",
    "                           for _ in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data_rec_10000.mean() - 2*data_rec_10000.std())\n",
    "print (data_rec_10000.mean() + 2*data_rec_10000.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_rec_10000, 15, color=clr1);\n",
    "plt.xlabel(r'$cost_B - cost_A$')\n",
    "plt.ylabel('count');\n",
    "save_fig(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_measurement(num_measurements):\n",
    "    measurements = [randomized_experiment_complex(\n",
    "                                num_nuisance_factors=100)\n",
    "                    for _ in range(num_measurements)]\n",
    "    return np.array(measurements).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17);\n",
    "aggregate_measurement(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_mean(data, num_measurements):\n",
    "    # Compute means by resampling from data rather than generating new data.\n",
    "    # This is done here just to save the time that would be required to\n",
    "    #  generate new data on each call to this function.\n",
    "    i = np.random.randint(data.shape[0], size=(num_measurements,))\n",
    "    return data[i].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17);\n",
    "data_10 = np.array([bootstrap_mean(data_rec_10000, 10) for _ in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_rec_10000,15,color=\"#333333\");\n",
    "plt.hist(data_10,15,color=\"#777777\");\n",
    "plt.xlabel(r'$cost_B - cost_A$')\n",
    "plt.ylabel('count');\n",
    "plt.legend(['single measurement', 'average of\\n10 measurements'], fontsize=8);\n",
    "save_fig(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17);\n",
    "data_100 = np.array([bootstrap_mean(data_rec_10000, 100) for _ in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_rec_10000,15,color=\"#333333\");\n",
    "plt.hist(data_10,15,color=\"#777777\");\n",
    "plt.hist(data_100,15,color=\"#BBBBBB\");\n",
    "plt.xlabel(r'$cost_B - cost_A$')\n",
    "plt.ylabel('count');\n",
    "plt.legend(['single measurement', 'average of\\n10 measurements',\n",
    "           'average of\\n100 measurements'], fontsize=8);\n",
    "save_fig(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Quantify variability with standard error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_SD1():\n",
    "    measurements = [randomized_experiment_complex(num_nuisance_factors=100)\n",
    "                        for _ in range(100)]\n",
    "    return np.array(measurements).std()\n",
    "\n",
    "np.random.seed(17); calc_SD1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_vs_N(data_rec_10000, expectation, hline=None, histogram=True, max_N=10000):\n",
    "    np.random.seed(17);\n",
    "    N = np.arange(1, max_N)\n",
    "    if histogram:\n",
    "        data = []\n",
    "        for n in N:\n",
    "            for _ in range(10):\n",
    "                m = bootstrap_mean(data_rec_10000, n)\n",
    "                data.append( (n,m) )\n",
    "        data = np.array(data)\n",
    "        plt.plot(data[:,0], expectation + data[:,1] + 1, '.', markersize=1, color=\"#AAAAAA\")\n",
    "        \n",
    "    sd = data_rec_10000.std()\n",
    "    if max_N <= 100:\n",
    "        fmt = \".--\"\n",
    "    else:\n",
    "        fmt = \"--\"\n",
    "        \n",
    "    plt.plot(N, expectation + sd/np.sqrt(N), fmt, color=\"#333333\")\n",
    "    plt.plot(N, expectation - sd/np.sqrt(N), fmt, color=\"#333333\")\n",
    "    plt.xlabel('number of measurements, N')\n",
    "\n",
    "    if histogram:\n",
    "        plt.ylabel('$cost_B - cost_A$')\n",
    "        plt.legend(['aggreagate (average of N) measurements', 'standard error (S.E.)'], fontsize=8,\n",
    "                  loc = 'lower right');\n",
    "    else:\n",
    "        plt.ylabel('SE of $cost_B - cost_A$')\n",
    "\n",
    "    if hline is not None:\n",
    "        horizonal_line(hline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_vs_N(data_rec_10000, expectation=-1)\n",
    "save_fig(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Minimize measurement costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_vs_N(data_rec_10000, expectation=-1, hline=0)\n",
    "save_fig(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PS = .3\n",
    "se_vs_N(data_rec_10000, expectation=-PS, hline=0, histogram=False)\n",
    "save_fig(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_vs_N(data_rec_10000, expectation=-PS, hline=0, histogram=False,\n",
    "        max_N=10)\n",
    "save_fig(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_measurement_model(num_measurements):\n",
    "    PS = .3\n",
    "    SD1 = .707\n",
    "    measurements = -PS + SD1*np.random.normal(size=(num_measurements,))\n",
    "    return measurements.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.random.normal(size=(10000,)), 25, color=\"#333333\");\n",
    "plt.hist(4 + .5*np.random.normal(size=(10000,)), 25, color=\"#777777\");\n",
    "c = plt.axis()\n",
    "plt.axis([c[0], c[1], 0, 1700])\n",
    "plt.legend(['np.random.normal(size=(10000,))', '4 + .5*np.random.normal(size=(10000,))'], loc='upper left');\n",
    "save_fig(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "aggregate_measurement_model(num_measurements=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_false_negative(N):\n",
    "    samples  = np.array([aggregate_measurement_model(num_measurements=N) for _ in range(10000)])\n",
    "    return len(np.where(samples > 0)[0]) / len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "probability_false_negative(N=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_measurement_model_t(num_measurements, num_samples_stddev):\n",
    "    PS = .3\n",
    "    SD1 = .707\n",
    "    measurements = -PS + SD1*np.random.standard_t(df=num_samples_stddev-1, size=(num_measurements,))\n",
    "    return measurements.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_false_negative_t(N, num_samples_stddev):\n",
    "    samples  = np.array([aggregate_measurement_model_t(num_measurements=N,\n",
    "                                                      num_samples_stddev=num_samples_stddev) for _ in range(10000)])\n",
    "    return len(np.where(samples>0)[0]) / len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "probability_false_negative_t(N=6, num_samples_stddev=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_fn_fp():\n",
    "    SD1 = .707\n",
    "    PS = .3\n",
    "    data = []\n",
    "    thresh = None\n",
    "    best_N = None\n",
    "    for N in range(1,100):\n",
    "        SE = SD1 / np.sqrt(N)\n",
    "        upper_fp_alpha = 0 + 1.96*SE\n",
    "        lower_fp_alpha = 0 - 1.96*SE\n",
    "        upper_fn_beta = -PS + .84*SE\n",
    "        lower_fn_beta = -PS - .84*SE\n",
    "        if thresh is None and upper_fn_beta <= lower_fp_alpha:\n",
    "            thresh = (upper_fn_beta + lower_fp_alpha) / 2\n",
    "            best_N = N\n",
    "        data.append( (N, upper_fp_alpha, lower_fp_alpha, upper_fn_beta, lower_fn_beta) )\n",
    "    return np.array(data), thresh, best_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, thresh, best_N = overlap_fn_fp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(data[:,0], data[:,3], data[:,4], color=clr3, alpha=.75, linewidth=1)\n",
    "\n",
    "\n",
    "plt.xlabel('number of measurements, N')\n",
    "plt.ylabel('$cost_B - cost_A$')\n",
    "plt.legend([r'Reject false negatives with $\\beta = .20$'])\n",
    "save_fig(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(data[:,0], data[:,1], data[:,2], color=clr2, alpha=.75, linewidth=1)\n",
    "\n",
    "\n",
    "plt.xlabel('number of measurements, N')\n",
    "plt.ylabel('$cost_B - cost_A$')\n",
    "plt.legend([r'Reject false positives with $\\alpha = .05$'])\n",
    "\n",
    "save_fig(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (thresh, best_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, thresh, best_N = overlap_fn_fp()\n",
    "plt.fill_between(data[:,0], data[:,1], data[:,2], color=clr2, alpha=.75, linewidth=1)\n",
    "plt.fill_between(data[:,0], data[:,3], data[:,4], color=clr3, alpha=.75, linewidth=1)\n",
    "\n",
    "circle = mpl.patches.Ellipse( (best_N, thresh), .2*25, .2, color=clr1, fill=False)#, transform=plt.gca().transAxes)\n",
    "plt.gcf().gca().add_artist(circle)\n",
    "\n",
    "plt.xlabel('number of measurements, N')\n",
    "plt.ylabel('$cost_B - cost_A$')\n",
    "plt.legend([r'$\\alpha = .05$', r'$\\beta = .20$'], fontsize=8,\n",
    "          loc = 'lower right');\n",
    "\n",
    "save_fig(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_false_negative_t(20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3\tRun and analyze the A/B test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2.1\n",
    "alpha = .05\n",
    "for N_small in [10, 30, 100, 300, 1000]:\n",
    "    k = scipy.stats.t.ppf(1-alpha, df=N_small)\n",
    "    print (N_small, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4\tEarly stopping produces invalid conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_stat_vs_sample():\n",
    "    measurements = np.array([])\n",
    "    t_stat = []\n",
    "    threshold = []\n",
    "    alpha=.05\n",
    "    for df in range(1, 100):\n",
    "        measurements = np.append(measurements, np.random.normal())\n",
    "        if df > 1:\n",
    "            mu = measurements[:df].mean()\n",
    "            sd = measurements[:df].std()\n",
    "            t = np.sqrt(df) * mu / sd\n",
    "        else:\n",
    "            t = np.nan\n",
    "        t_stat.append(t)\n",
    "        threshold.append(scipy.stats.t.ppf(1-alpha, df=df))\n",
    "    t_stat = np.array(t_stat)\n",
    "    threshold = np.array(threshold)\n",
    "    return t_stat, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 70366 #np.random.randint(100000)\n",
    "np.random.seed(seed)\n",
    "\n",
    "t_stat, threshold = t_stat_vs_sample()\n",
    "\n",
    "plt.plot(threshold, '--k', color=clr1)\n",
    "plt.plot(t_stat, color=clr2, linewidth=1);\n",
    "plt.plot(-threshold, '--k', color=clr1)\n",
    "plt.xlabel('sample')\n",
    "plt.legend(['threshold, k', 't statistic'])\n",
    "\n",
    "save_fig(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positive_rates():\n",
    "    num_ab_tests = 100\n",
    "    fp_at_end = 0\n",
    "    fp_early_stopping = 0\n",
    "    for _ in range(num_ab_tests):\n",
    "        t_stat, threshold = t_stat_vs_sample() \n",
    "        if t_stat[-1] > threshold[-1]:\n",
    "            fp_at_end += 1\n",
    "        i = np.where(t_stat[1:] > threshold[1:])[0]\n",
    "        if len(i) > 0:\n",
    "            fp_early_stopping += 1\n",
    "    return fp_at_end / num_ab_tests, fp_early_stopping / num_ab_tests\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17);false_positive_rates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster verions of t_stat_vs_sample() and false_positive_rates() used to\n",
    "#  generate data for Figure 2.19.  The original versions were easier to use for\n",
    "#  teaching, but too slow to generate the figure in a reasonable amount of time.\n",
    "def t_stat_vs_sample_fast(N):\n",
    "    measurements = np.random.normal(size=(N-1,))\n",
    "    N = np.arange(2, N+1)\n",
    "    sx = np.cumsum(measurements)\n",
    "    sxx = np.cumsum(measurements**2)\n",
    "    mu = sx/N\n",
    "    sd = np.sqrt(sxx/N - mu**2)\n",
    "    t_stats = np.sqrt(N) * mu/sd\n",
    "    return t_stats\n",
    "\n",
    "def false_positive_rates_fast(N):\n",
    "    num_ab_tests = 10000\n",
    "    fp_at_end = 0\n",
    "    fp_early_stopping = 0\n",
    "    for _ in range(num_ab_tests):\n",
    "        t_stat = t_stat_vs_sample_fast(N)    \n",
    "        if t_stat[-1] > 1.65:\n",
    "            fp_at_end += 1\n",
    "        i = np.where(t_stat > 1.65)[0]\n",
    "        if len(i) > 0:\n",
    "            fp_early_stopping += 1\n",
    "    return fp_at_end / num_ab_tests, fp_early_stopping / num_ab_tests\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "false_positive_rates_fast(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "fpr = []\n",
    "for N in [10, 30, 100, 300, 1000, 3000, int(1e4), int(3e4), int(1e5), int(3e5), int(1e6)]:\n",
    "    fp_N = false_positive_rates_fast(N)[1]\n",
    "    print (N, fp_N)\n",
    "    fpr.append( (N, fp_N)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = np.array(fpr)\n",
    "plt.semilogx(fpr[:,0], fpr[:,1], '.--', color=clr1);\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('false positive rate')\n",
    "save_fig(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
